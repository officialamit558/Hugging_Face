# HUGGING FACE

Cover many topic like...
1) Define the Large Language Model.
2) Detail explaintions of the Transfomers architecture.
3) Learn transfer learning method like fine-tunning and feature extractions method.
4) In fine tunning , we actually freez the fully connnected layer in cnn or some top layer which are trained on the other dataset. Therefore remove the some top layer and the own traineable or new dataset on which we want the required result.
5) In the feature extraction , we actually not only remove the top layer but also freez the some main layers of the transfoerms or other architecture , like in the CNN we actually take from the flatten layers of the cnn architecture.
6) Now move towards the more adveture part of the NLP model.
7) Explore the many method in the NLP to solved the complex problem in more effecient way and less time consuming or we can say that , required less computational cost.
8) Understant the deep architecture of the transformers.
9) Learn about the different component of the encoder and decoder part of the transfomers.
10) Encoders parts consist of the many small component like multi-head-attention + Add Norm + Full connected layers + Add Softmax Function -> Outputs transfer to the Decoders part.
11) Decoders architecture contains the positional-encoding layers + Dynamic - embeding + Multi head attention + Cross multi-head masked attentions + add norm + fully connectd layers + apply the softmax layers -> Than produced the outputs of the given promts.
